{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89520b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "670c3bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SystemGraphBuilder:\n",
    "    \"\"\"\n",
    "    Builds a directed graph from extracted system components and computes\n",
    "    integration risk features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the graph builder.\"\"\"\n",
    "        self.graph = None\n",
    "        self.components = []\n",
    "        self.features = {}\n",
    "    \n",
    "    def load_components(self, json_file: str) -> None:\n",
    "        \"\"\"\n",
    "        Load extracted components from JSON file.\n",
    "        \n",
    "        Args:\n",
    "            json_file: Path to extracted_components.json\n",
    "        \"\"\"\n",
    "        print(f\"ðŸ“¥ Loading components from {json_file}...\")\n",
    "        with open('extracted_components.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.components = data['components']\n",
    "        print(f\"âœ… Loaded {len(self.components)} components\")\n",
    "    \n",
    "    def build_graph(self) -> nx.DiGraph:\n",
    "        \"\"\"\n",
    "        Build a directed graph from components.\n",
    "        \n",
    "        Nodes represent components.\n",
    "        Edges represent dependencies (A â†’ B means A depends on B).\n",
    "        \n",
    "        Returns:\n",
    "            NetworkX directed graph\n",
    "        \"\"\"\n",
    "        print(\"ðŸ”¨ Building dependency graph...\")\n",
    "        \n",
    "        self.graph = nx.DiGraph()\n",
    "        \n",
    "        # Add nodes with attributes\n",
    "        for comp in self.components:\n",
    "            self.graph.add_node(\n",
    "                comp['id'],\n",
    "                name=comp['name'],\n",
    "                type=comp['type'],\n",
    "                technology=comp['technology'],\n",
    "                purpose=comp['purpose'],\n",
    "                criticality=comp['criticality']\n",
    "            )\n",
    "        \n",
    "        # Add edges (dependencies)\n",
    "        for comp in self.components:\n",
    "            source = comp['id']\n",
    "            for dependency in comp.get('dependencies', []):\n",
    "                # Edge from source to dependency (source depends on dependency)\n",
    "                if dependency in self.graph.nodes:\n",
    "                    self.graph.add_edge(source, dependency)\n",
    "                else:\n",
    "                    print(f\"âš ï¸  Warning: Dependency '{dependency}' not found for '{source}'\")\n",
    "        \n",
    "        print(f\"âœ… Graph built: {self.graph.number_of_nodes()} nodes, {self.graph.number_of_edges()} edges\")\n",
    "        \n",
    "        return self.graph\n",
    "    \n",
    "    def compute_features(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Compute graph-theoretic features for each node.\n",
    "        \n",
    "        Features computed:\n",
    "        - In-degree: Number of components depending on this component\n",
    "        - Out-degree: Number of components this component depends on\n",
    "        - Total degree: In + Out degree\n",
    "        - Betweenness centrality: How often node appears on shortest paths\n",
    "        - Closeness centrality: How close node is to all other nodes\n",
    "        - PageRank: Importance based on dependency structure\n",
    "        - Clustering coefficient: Local connectivity\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping component IDs to feature vectors\n",
    "        \"\"\"\n",
    "        print(\"ðŸ“Š Computing graph features...\")\n",
    "        \n",
    "        if self.graph is None:\n",
    "            raise ValueError(\"Graph not built. Call build_graph() first.\")\n",
    "        \n",
    "        # Compute centrality measures\n",
    "        betweenness = nx.betweenness_centrality(self.graph)\n",
    "        closeness = nx.closeness_centrality(self.graph)\n",
    "        pagerank = nx.pagerank(self.graph)\n",
    "        clustering = nx.clustering(self.graph.to_undirected())\n",
    "        \n",
    "        # Build feature dictionary for each node\n",
    "        self.features = {}\n",
    "        for node in self.graph.nodes():\n",
    "            self.features[node] = {\n",
    "                'in_degree': self.graph.in_degree(node),\n",
    "                'out_degree': self.graph.out_degree(node),\n",
    "                'total_degree': self.graph.degree(node),\n",
    "                'betweenness_centrality': betweenness[node],\n",
    "                'closeness_centrality': closeness[node],\n",
    "                'pagerank': pagerank[node],\n",
    "                'clustering_coefficient': clustering[node],\n",
    "                'criticality': self.graph.nodes[node]['criticality'],\n",
    "                'type': self.graph.nodes[node]['type']\n",
    "            }\n",
    "        \n",
    "        print(\"âœ… Features computed for all nodes\")\n",
    "        \n",
    "        return self.features\n",
    "    \n",
    "    def identify_critical_nodes(self, top_n: int = 5) -> List[Tuple[str, Dict]]:\n",
    "        \"\"\"\n",
    "        Identify most critical nodes based on multiple metrics.\n",
    "        \n",
    "        Args:\n",
    "            top_n: Number of top critical nodes to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (node_id, features) tuples sorted by criticality score\n",
    "        \"\"\"\n",
    "        print(f\"ðŸŽ¯ Identifying top {top_n} critical nodes...\")\n",
    "        \n",
    "        if not self.features:\n",
    "            raise ValueError(\"Features not computed. Call compute_features() first.\")\n",
    "        \n",
    "        # Compute composite criticality score\n",
    "        # Weighted combination of multiple features\n",
    "        criticality_scores = {}\n",
    "        \n",
    "        for node, features in self.features.items():\n",
    "            # Normalize and weight features\n",
    "            score = (\n",
    "                0.3 * features['in_degree'] +           # High in-degree = many depend on it\n",
    "                0.2 * features['betweenness_centrality'] +  # High betweenness = bottleneck\n",
    "                0.2 * features['pagerank'] +             # High pagerank = structurally important\n",
    "                0.15 * features['out_degree'] +          # High out-degree = depends on many\n",
    "                0.15 * (1 - features['clustering_coefficient'])  # Low clustering = less redundancy\n",
    "            )\n",
    "            criticality_scores[node] = score\n",
    "        \n",
    "        # Sort by score\n",
    "        sorted_nodes = sorted(\n",
    "            criticality_scores.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        top_nodes = [\n",
    "            (node, self.features[node]) \n",
    "            for node, score in sorted_nodes[:top_n]\n",
    "        ]\n",
    "        \n",
    "        print(f\"âœ… Identified {len(top_nodes)} critical nodes\")\n",
    "        \n",
    "        return top_nodes\n",
    "    \n",
    "    def analyze_failure_impact(self, node: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze the impact of a single node failure.\n",
    "        \n",
    "        Args:\n",
    "            node: Component ID to analyze\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with failure impact metrics\n",
    "        \"\"\"\n",
    "        if node not in self.graph.nodes:\n",
    "            raise ValueError(f\"Node '{node}' not found in graph\")\n",
    "        \n",
    "        # Components that directly depend on this node\n",
    "        dependents = list(self.graph.predecessors(node))\n",
    "        \n",
    "        # Components this node depends on (would also fail)\n",
    "        dependencies = list(self.graph.successors(node))\n",
    "        \n",
    "        # Cascade analysis: what fails if this node fails\n",
    "        cascade = self._compute_failure_cascade(node)\n",
    "        \n",
    "        return {\n",
    "            'node': node,\n",
    "            'name': self.graph.nodes[node]['name'],\n",
    "            'direct_dependents': len(dependents),\n",
    "            'dependent_nodes': dependents,\n",
    "            'dependencies': dependencies,\n",
    "            'cascade_size': len(cascade),\n",
    "            'cascade_nodes': cascade,\n",
    "            'cascade_percentage': len(cascade) / self.graph.number_of_nodes() * 100\n",
    "        }\n",
    "    \n",
    "    def _compute_failure_cascade(self, failed_node: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Compute which nodes would fail if given node fails.\n",
    "        Uses BFS to find all reachable predecessors.\n",
    "        \"\"\"\n",
    "        cascade = set()\n",
    "        queue = [failed_node]\n",
    "        visited = {failed_node}\n",
    "        \n",
    "        while queue:\n",
    "            current = queue.pop(0)\n",
    "            # All nodes that depend on current node\n",
    "            for predecessor in self.graph.predecessors(current):\n",
    "                if predecessor not in visited:\n",
    "                    visited.add(predecessor)\n",
    "                    cascade.add(predecessor)\n",
    "                    queue.append(predecessor)\n",
    "        \n",
    "        return list(cascade)\n",
    "    \n",
    "    def visualize_graph(self, output_file: str = \"system_graph.png\",\n",
    "                       highlight_nodes: List[str] = None) -> None:\n",
    "        \"\"\"\n",
    "        Visualize the dependency graph.\n",
    "        \n",
    "        Args:\n",
    "            output_file: Output filename for the graph visualization\n",
    "            highlight_nodes: List of node IDs to highlight in red\n",
    "        \"\"\"\n",
    "        print(f\"ðŸŽ¨ Visualizing graph...\")\n",
    "        \n",
    "        if self.graph is None:\n",
    "            raise ValueError(\"Graph not built. Call build_graph() first.\")\n",
    "        \n",
    "        plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        # Use spring layout for better visualization\n",
    "        pos = nx.spring_layout(self.graph, k=2, iterations=50, seed=42)\n",
    "        \n",
    "        # Color nodes by type\n",
    "        type_colors = {\n",
    "            'application': '#3B82F6',  # Blue\n",
    "            'infrastructure': '#10B981',  # Green\n",
    "            'storage': '#F59E0B',  # Orange\n",
    "            'network': '#8B5CF6',  # Purple\n",
    "            'security': '#EF4444'  # Red\n",
    "        }\n",
    "        \n",
    "        node_colors = []\n",
    "        for node in self.graph.nodes():\n",
    "            if highlight_nodes and node in highlight_nodes:\n",
    "                node_colors.append('#DC2626')  # Bright red for highlighted\n",
    "            else:\n",
    "                node_type = self.graph.nodes[node]['type']\n",
    "                node_colors.append(type_colors.get(node_type, '#6B7280'))\n",
    "        \n",
    "        # Size nodes by in-degree (how many depend on them)\n",
    "        node_sizes = [\n",
    "            300 + self.graph.in_degree(node) * 150 \n",
    "            for node in self.graph.nodes()\n",
    "        ]\n",
    "        \n",
    "        # Draw graph\n",
    "        nx.draw_networkx_nodes(\n",
    "            self.graph, pos,\n",
    "            node_color=node_colors,\n",
    "            node_size=node_sizes,\n",
    "            alpha=0.9,\n",
    "            edgecolors='white',\n",
    "            linewidths=2\n",
    "        )\n",
    "        \n",
    "        nx.draw_networkx_edges(\n",
    "            self.graph, pos,\n",
    "            edge_color='#9CA3AF',\n",
    "            arrows=True,\n",
    "            arrowsize=15,\n",
    "            arrowstyle='->',\n",
    "            width=1.5,\n",
    "            alpha=0.6,\n",
    "            connectionstyle='arc3,rad=0.1'\n",
    "        )\n",
    "        \n",
    "        # Labels\n",
    "        labels = {\n",
    "            node: self.graph.nodes[node]['name'].replace(' ', '\\n') \n",
    "            for node in self.graph.nodes()\n",
    "        }\n",
    "        nx.draw_networkx_labels(\n",
    "            self.graph, pos,\n",
    "            labels,\n",
    "            font_size=7,\n",
    "            font_weight='bold'\n",
    "        )\n",
    "        \n",
    "        plt.title(\"LMS System Dependency Graph\\n(Node size = # of dependents)\", \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"âœ… Graph saved to {output_file}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def save_features(self, output_file: str = \"graph_features.json\") -> None:\n",
    "        \"\"\"\n",
    "        Save computed features to JSON file.\n",
    "        \n",
    "        Args:\n",
    "            output_file: Output filename\n",
    "        \"\"\"\n",
    "        if not self.features:\n",
    "            raise ValueError(\"Features not computed. Call compute_features() first.\")\n",
    "        \n",
    "        output = {\n",
    "            'timestamp': '2025-10-30',\n",
    "            'graph_statistics': {\n",
    "                'num_nodes': self.graph.number_of_nodes(),\n",
    "                'num_edges': self.graph.number_of_edges(),\n",
    "                'density': nx.density(self.graph),\n",
    "                'is_dag': nx.is_directed_acyclic_graph(self.graph)\n",
    "            },\n",
    "            'node_features': self.features\n",
    "        }\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(output, f, indent=2)\n",
    "        \n",
    "        print(f\"ðŸ’¾ Features saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd2a0830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LMS SYSTEM GRAPH BUILDER\n",
      "============================================================\n",
      "\n",
      "Prerequisites:\n",
      "âœ“ extracted_components.json (from ChatGPT/Claude)\n",
      "âœ“ JSON validated and verified\n",
      "\n",
      "ðŸ“¥ Loading components from extracted_components.json...\n",
      "âœ… Loaded 22 components\n",
      "ðŸ”¨ Building dependency graph...\n",
      "âœ… Graph built: 22 nodes, 38 edges\n",
      "ðŸ“Š Computing graph features...\n",
      "âœ… Features computed for all nodes\n",
      "ðŸŽ¯ Identifying top 5 critical nodes...\n",
      "âœ… Identified 5 critical nodes\n",
      "\n",
      "============================================================\n",
      "TOP 5 CRITICAL COMPONENTS\n",
      "============================================================\n",
      "1. Primary Database Cluster\n",
      "   In-degree: 10 (components depending on this)\n",
      "   Out-degree: 0 (dependencies)\n",
      "   Betweenness: 0.000\n",
      "   PageRank: 0.180\n",
      "\n",
      "2. Cache Layer\n",
      "   In-degree: 6 (components depending on this)\n",
      "   Out-degree: 0 (dependencies)\n",
      "   Betweenness: 0.000\n",
      "   PageRank: 0.111\n",
      "\n",
      "3. Object Storage\n",
      "   In-degree: 5 (components depending on this)\n",
      "   Out-degree: 0 (dependencies)\n",
      "   Betweenness: 0.000\n",
      "   PageRank: 0.092\n",
      "\n",
      "4. API Gateway Service\n",
      "   In-degree: 1 (components depending on this)\n",
      "   Out-degree: 7 (dependencies)\n",
      "   Betweenness: 0.024\n",
      "   PageRank: 0.039\n",
      "\n",
      "5. Application Load Balancer\n",
      "   In-degree: 1 (components depending on this)\n",
      "   Out-degree: 6 (dependencies)\n",
      "   Betweenness: 0.019\n",
      "   PageRank: 0.039\n",
      "\n",
      "============================================================\n",
      "FAILURE IMPACT ANALYSIS: Primary Database Cluster\n",
      "============================================================\n",
      "Direct dependents: 10\n",
      "Cascade size: 14 components\n",
      "Cascade percentage: 63.6% of system\n",
      "\n",
      "ðŸ’¾ Features saved to graph_features.json\n",
      "ðŸŽ¨ Visualizing graph...\n",
      "âœ… Graph saved to system_graph.png\n",
      "============================================================\n",
      "âœ… Graph building and feature computation complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution - demonstrates the graph building pipeline.\n",
    "    \n",
    "    NOTE: Before running this, ensure you have:\n",
    "    1. Created extracted_components.json using ChatGPT prompts\n",
    "    2. Validated the JSON structure\n",
    "    3. Verified all dependencies reference existing components\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"LMS SYSTEM GRAPH BUILDER\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nPrerequisites:\")\n",
    "    print(\"âœ“ extracted_components.json (from ChatGPT/Claude)\")\n",
    "    print(\"âœ“ JSON validated and verified\")\n",
    "    print()\n",
    "    \n",
    "    # Initialize builder\n",
    "    builder = SystemGraphBuilder()\n",
    "    \n",
    "    # Load components from extractor output (manually created using LLM)\n",
    "    builder.load_components('extracted_components.json')\n",
    "    \n",
    "    # Build graph\n",
    "    builder.build_graph()\n",
    "    \n",
    "    # Compute features\n",
    "    builder.compute_features()\n",
    "    \n",
    "    # Identify critical nodes\n",
    "    critical_nodes = builder.identify_critical_nodes(top_n=5)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TOP 5 CRITICAL COMPONENTS\")\n",
    "    print(\"=\"*60)\n",
    "    for i, (node, features) in enumerate(critical_nodes, 1):\n",
    "        print(f\"{i}. {builder.graph.nodes[node]['name']}\")\n",
    "        print(f\"   In-degree: {features['in_degree']} (components depending on this)\")\n",
    "        print(f\"   Out-degree: {features['out_degree']} (dependencies)\")\n",
    "        print(f\"   Betweenness: {features['betweenness_centrality']:.3f}\")\n",
    "        print(f\"   PageRank: {features['pagerank']:.3f}\")\n",
    "        print()\n",
    "    \n",
    "    # Analyze failure impact for top critical node\n",
    "    top_node = critical_nodes[0][0]\n",
    "    impact = builder.analyze_failure_impact(top_node)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"FAILURE IMPACT ANALYSIS: {impact['name']}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Direct dependents: {impact['direct_dependents']}\")\n",
    "    print(f\"Cascade size: {impact['cascade_size']} components\")\n",
    "    print(f\"Cascade percentage: {impact['cascade_percentage']:.1f}% of system\")\n",
    "    print()\n",
    "    \n",
    "    # Save results\n",
    "    builder.save_features()\n",
    "    \n",
    "    # Visualize graph (highlight critical nodes)\n",
    "    critical_node_ids = [node for node, _ in critical_nodes]\n",
    "    builder.visualize_graph(highlight_nodes=critical_node_ids)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"âœ… Graph building and feature computation complete!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca56aba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
